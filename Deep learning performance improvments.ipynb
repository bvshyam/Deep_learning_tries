{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 Improving performance Exercises Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "- Reload the IMDB data keeping only the first 20000 most common words\n",
    "- pad the reviews to a shorter length (eg. 70 or 80), this time make sure you keep the first part of the review if it's longer than the maximum length\n",
    "- re run the model (remember to set max_features correctly)\n",
    "- does it train faster this time?\n",
    "- do you get a better performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "skip_top = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data('/tmp/imdb.npz',\n",
    "                                                      num_words=max_features,\n",
    "                                                      start_char=1,\n",
    "                                                      oov_char=2,\n",
    "                                                      index_from=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_pad = pad_sequences(X_train, maxlen=maxlen, truncating='post')\n",
    "X_test_pad = pad_sequences(X_test, maxlen=maxlen, truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train_pad, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=2,\n",
    "          validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score, acc = model.evaluate(X_test_pad, y_test)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "- Reload the digits data as above\n",
    "- define a function repeated_training_reg_dropout that adds regularization and dropout to a fully connected network\n",
    "- compare the performance with/witouth dropout and regularization like we did for batch normalization\n",
    "- do you get a better performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dropout\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "y_cat = to_categorical(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def repeated_training_reg_dropout(X_train,\n",
    "                                  y_train,\n",
    "                                  X_test,\n",
    "                                  y_test,\n",
    "                                  units=512,\n",
    "                                  activation='sigmoid',\n",
    "                                  optimizer='sgd',\n",
    "                                  do_dropout=False,\n",
    "                                  rate=0.3,\n",
    "                                  kernel_regularizer='l2',\n",
    "                                  epochs=10,\n",
    "                                  repeats=3):\n",
    "    histories = []\n",
    "    \n",
    "    for repeat in range(repeats):\n",
    "        K.clear_session()\n",
    "\n",
    "        model = Sequential()\n",
    "        \n",
    "        # first fully connected layer\n",
    "        model.add(Dense(units,\n",
    "                        input_shape=X_train.shape[1:],\n",
    "                        kernel_initializer='normal',\n",
    "                        kernel_regularizer=kernel_regularizer,\n",
    "                        activation=activation))\n",
    "        if do_dropout:\n",
    "            model.add(Dropout(rate))\n",
    "\n",
    "        # second fully connected layer\n",
    "        model.add(Dense(units,\n",
    "                        kernel_initializer='normal',\n",
    "                        kernel_regularizer=kernel_regularizer,\n",
    "                        activation=activation))\n",
    "        if do_dropout:\n",
    "            model.add(Dropout(rate))\n",
    "\n",
    "        # third fully connected layer\n",
    "        model.add(Dense(units,\n",
    "                        kernel_initializer='normal',\n",
    "                        kernel_regularizer=kernel_regularizer,\n",
    "                        activation=activation))\n",
    "        if do_dropout:\n",
    "            model.add(Dropout(rate))\n",
    "\n",
    "        # output layer\n",
    "        model.add(Dense(10, activation='softmax'))\n",
    "        \n",
    "        model.compile(optimizer,\n",
    "                      'categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        h = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, verbose=0)\n",
    "        histories.append([h.history['acc'], h.history['val_acc']])\n",
    "        print(repeat, end=' ')\n",
    "\n",
    "    histories = np.array(histories)\n",
    "    \n",
    "    # calculate mean and standard deviation across repeats:\n",
    "    mean_acc = histories.mean(axis=0)\n",
    "    std_acc = histories.std(axis=0)\n",
    "    print()\n",
    "    \n",
    "    return mean_acc[0], std_acc[0], mean_acc[1], std_acc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_acc, std_acc, mean_acc_val, std_acc_val = repeated_training_reg_dropout(X_train,\n",
    "                                                                             y_train,\n",
    "                                                                             X_test,\n",
    "                                                                             y_test,\n",
    "                                                                             do_dropout=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_acc_do, std_acc_do, mean_acc_val_do, std_acc_val_do = repeated_training_reg_dropout(X_train,\n",
    "                                                                                         y_train,\n",
    "                                                                                         X_test,\n",
    "                                                                                         y_test,\n",
    "                                                                                         do_dropout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_mean_std(m, s):\n",
    "    plt.plot(m)\n",
    "    plt.fill_between(range(len(m)), m-s, m+s, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_mean_std(mean_acc, std_acc)\n",
    "plot_mean_std(mean_acc_val, std_acc_val)\n",
    "plot_mean_std(mean_acc_do, std_acc_do)\n",
    "plot_mean_std(mean_acc_val_do, std_acc_val_do)\n",
    "plt.ylim(0, 1.01)\n",
    "plt.title(\"Dropout and Regularization Accuracy\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Test', 'Train with Dropout and Regularization', 'Test with Dropout and Regularization'], loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "This is a very long and complex exercise, that should give you an idea of a real world scenario. Feel free to look at the solution if you feel lost. Also, feel free to run this on Floyd with a GPU, in which case you don't need to download the data.\n",
    "\n",
    "If you are running this locally, download and unpack the male/female pictures from [here](https://www.dropbox.com/s/nov493om2jmh2gp/male_female.tgz?dl=0). These images and labels were obtained from [Crowdflower](https://www.crowdflower.com/data-for-everyone/).\n",
    "\n",
    "Your goal is to build an image classifier that will recognize the gender of a person from pictures.\n",
    "\n",
    "- Have a look at the directory structure and inspect a couple of pictures\n",
    "- Design a model that will take a color image of size 64x64 as input and return a binary output (female=0/male=1)\n",
    "- Feel free to introduce any regularization technique in your model (Dropout, Batch Normalization, Weight Regularization)\n",
    "- Compile your model with an optimizer of your choice\n",
    "- Using `ImageDataGenerator`, define a train generator that will augment your images with some geometric transformations. Feel free to choose the parameters that make sense to you.\n",
    "- Define also a test generator, whose only purpose is to rescale the pixels by 1./255\n",
    "- use the function `flow_from_directory` to generate batches from the train and test folders. Make sure you set the `target_size` to 64x64.\n",
    "- Use the `model.fit_generator` function to fit the model on the batches generated from the ImageDataGenerator. Since you are streaming and augmenting the data in real time you will have to decide how many batches make an epoch and how many epochs you want to run\n",
    "- Train your model (you should get to at least 85% accuracy)\n",
    "- Once you are satisfied with your training, check a few of the misclassified pictures. Are those sensible errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if you are running this on floydhub.com, execute this cell and skip the next\n",
    "#data_path = /input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you are running this locally\n",
    "# uncomment the next 3 lines to download, extract and set the data path:\n",
    "# !wget https://www.dropbox.com/s/nov493om2jmh2gp/male_female.tgz?dl=0 -O male_female.tgz\n",
    "# !tar -xzvf male_female.tgz\n",
    "#data_path = '../data/male_female'\n",
    "data_path = 'C:/Users/paperspace/Google Drive/CUNY/Courses/Archive/zero_to_deep_learning_udemy/data/male_female/male_female/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import BatchNormalization\n",
    "from itertools import islice\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import keras.backend as K\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 31, 31, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 29, 29, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 12, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 6, 6, 64)          256       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               295040    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 352,129\n",
      "Trainable params: 351,809\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(rescale = 1./255,\n",
    "                               width_shift_range=0.1,\n",
    "                               height_shift_range=0.1,\n",
    "                               rotation_range = 10,\n",
    "                               shear_range = 0.2,\n",
    "                               zoom_range = 0.2,\n",
    "                               horizontal_flip = True)\n",
    "\n",
    "test_gen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11663 images belonging to 2 classes.\n",
      "Found 2920 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train = train_gen.flow_from_directory(data_path + '/train',\n",
    "                                      target_size = (64, 64),\n",
    "                                      batch_size = 16,\n",
    "                                      class_mode = 'binary')\n",
    "\n",
    "test = test_gen.flow_from_directory(data_path + '/test',\n",
    "                                    target_size = (64, 64),\n",
    "                                    batch_size = 16,\n",
    "                                    class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "800/800 [==============================] - 293s 366ms/step - loss: 0.6643 - acc: 0.6742 - val_loss: 0.5788 - val_acc: 0.7099\n",
      "Epoch 2/200\n",
      "800/800 [==============================] - 250s 312ms/step - loss: 0.4997 - acc: 0.7451 - val_loss: 0.4610 - val_acc: 0.7713\n",
      "Epoch 3/200\n",
      "800/800 [==============================] - 244s 305ms/step - loss: 0.4585 - acc: 0.7724 - val_loss: 0.6169 - val_acc: 0.7368\n",
      "Epoch 4/200\n",
      "800/800 [==============================] - 246s 308ms/step - loss: 0.4329 - acc: 0.7896 - val_loss: 0.4752 - val_acc: 0.7569\n",
      "Epoch 5/200\n",
      "800/800 [==============================] - 240s 301ms/step - loss: 0.4166 - acc: 0.8007 - val_loss: 0.4111 - val_acc: 0.8048\n",
      "Epoch 6/200\n",
      "800/800 [==============================] - 242s 303ms/step - loss: 0.3985 - acc: 0.8137 - val_loss: 0.3832 - val_acc: 0.8164\n",
      "Epoch 7/200\n",
      "800/800 [==============================] - 246s 307ms/step - loss: 0.3862 - acc: 0.8120 - val_loss: 0.3923 - val_acc: 0.8145\n",
      "Epoch 8/200\n",
      "800/800 [==============================] - 247s 309ms/step - loss: 0.3673 - acc: 0.8295 - val_loss: 0.3590 - val_acc: 0.8280\n",
      "Epoch 9/200\n",
      "800/800 [==============================] - 253s 316ms/step - loss: 0.3553 - acc: 0.8344 - val_loss: 0.4380 - val_acc: 0.8105\n",
      "Epoch 10/200\n",
      "800/800 [==============================] - 250s 313ms/step - loss: 0.3389 - acc: 0.8385 - val_loss: 0.3915 - val_acc: 0.8127\n",
      "Epoch 11/200\n",
      "800/800 [==============================] - 253s 316ms/step - loss: 0.3313 - acc: 0.8484 - val_loss: 0.3770 - val_acc: 0.8239\n",
      "Epoch 12/200\n",
      "800/800 [==============================] - 259s 324ms/step - loss: 0.3294 - acc: 0.8472 - val_loss: 0.3240 - val_acc: 0.8503\n",
      "Epoch 13/200\n",
      "800/800 [==============================] - 241s 302ms/step - loss: 0.3128 - acc: 0.8553 - val_loss: 0.3836 - val_acc: 0.8233\n",
      "Epoch 14/200\n",
      "800/800 [==============================] - 244s 305ms/step - loss: 0.3006 - acc: 0.8626 - val_loss: 0.3348 - val_acc: 0.8471\n",
      "Epoch 15/200\n",
      "800/800 [==============================] - 258s 323ms/step - loss: 0.3013 - acc: 0.8598 - val_loss: 0.3081 - val_acc: 0.8575\n",
      "Epoch 16/200\n",
      "800/800 [==============================] - 253s 316ms/step - loss: 0.2933 - acc: 0.8659 - val_loss: 0.3429 - val_acc: 0.8318\n",
      "Epoch 17/200\n",
      "800/800 [==============================] - 247s 309ms/step - loss: 0.2861 - acc: 0.8659 - val_loss: 0.3279 - val_acc: 0.8452\n",
      "Epoch 18/200\n",
      "800/800 [==============================] - 266s 332ms/step - loss: 0.2759 - acc: 0.8731 - val_loss: 0.3716 - val_acc: 0.8452\n",
      "Epoch 19/200\n",
      "800/800 [==============================] - 244s 305ms/step - loss: 0.2791 - acc: 0.8701 - val_loss: 0.3207 - val_acc: 0.8593\n",
      "Epoch 20/200\n",
      "800/800 [==============================] - 245s 307ms/step - loss: 0.2731 - acc: 0.8758 - val_loss: 0.3047 - val_acc: 0.8637\n",
      "Epoch 21/200\n",
      "800/800 [==============================] - 251s 314ms/step - loss: 0.2656 - acc: 0.8815 - val_loss: 0.2969 - val_acc: 0.8694\n",
      "Epoch 22/200\n",
      "800/800 [==============================] - 263s 328ms/step - loss: 0.2628 - acc: 0.8830 - val_loss: 0.3141 - val_acc: 0.8543\n",
      "Epoch 23/200\n",
      "800/800 [==============================] - 243s 304ms/step - loss: 0.2597 - acc: 0.8842 - val_loss: 0.3143 - val_acc: 0.8518\n",
      "Epoch 24/200\n",
      "800/800 [==============================] - 247s 309ms/step - loss: 0.2618 - acc: 0.8794 - val_loss: 0.2786 - val_acc: 0.8728\n",
      "Epoch 25/200\n",
      "800/800 [==============================] - 246s 308ms/step - loss: 0.2528 - acc: 0.8871 - val_loss: 0.2940 - val_acc: 0.8656\n",
      "Epoch 26/200\n",
      "800/800 [==============================] - 241s 301ms/step - loss: 0.2520 - acc: 0.8896 - val_loss: 0.2624 - val_acc: 0.8860\n",
      "Epoch 27/200\n",
      "800/800 [==============================] - 242s 303ms/step - loss: 0.2517 - acc: 0.8894 - val_loss: 0.2883 - val_acc: 0.8662\n",
      "Epoch 28/200\n",
      "800/800 [==============================] - 245s 306ms/step - loss: 0.2483 - acc: 0.8880 - val_loss: 0.2864 - val_acc: 0.8709\n",
      "Epoch 29/200\n",
      "800/800 [==============================] - 247s 308ms/step - loss: 0.2416 - acc: 0.8951 - val_loss: 0.3201 - val_acc: 0.8669\n",
      "Epoch 30/200\n",
      "800/800 [==============================] - 246s 308ms/step - loss: 0.2352 - acc: 0.8950 - val_loss: 0.2991 - val_acc: 0.8759\n",
      "Epoch 31/200\n",
      "800/800 [==============================] - 250s 312ms/step - loss: 0.2415 - acc: 0.8928 - val_loss: 0.3076 - val_acc: 0.8496\n",
      "Epoch 32/200\n",
      "800/800 [==============================] - 257s 322ms/step - loss: 0.2276 - acc: 0.9007 - val_loss: 0.2959 - val_acc: 0.8728\n",
      "Epoch 33/200\n",
      "800/800 [==============================] - 260s 325ms/step - loss: 0.2291 - acc: 0.8965 - val_loss: 0.3127 - val_acc: 0.8684\n",
      "Epoch 34/200\n",
      "800/800 [==============================] - 259s 324ms/step - loss: 0.2277 - acc: 0.8998 - val_loss: 0.3208 - val_acc: 0.8578\n",
      "Epoch 35/200\n",
      "800/800 [==============================] - 257s 322ms/step - loss: 0.2278 - acc: 0.9007 - val_loss: 0.2963 - val_acc: 0.8778\n",
      "Epoch 36/200\n",
      "800/800 [==============================] - 269s 336ms/step - loss: 0.2165 - acc: 0.9034 - val_loss: 0.3128 - val_acc: 0.8612\n",
      "Epoch 37/200\n",
      "800/800 [==============================] - 265s 331ms/step - loss: 0.2219 - acc: 0.9023 - val_loss: 0.2889 - val_acc: 0.8828\n",
      "Epoch 38/200\n",
      "800/800 [==============================] - 264s 330ms/step - loss: 0.2209 - acc: 0.9050 - val_loss: 0.2867 - val_acc: 0.8719\n",
      "Epoch 39/200\n",
      "800/800 [==============================] - 277s 347ms/step - loss: 0.2119 - acc: 0.9037 - val_loss: 0.2953 - val_acc: 0.8675\n",
      "Epoch 40/200\n",
      "800/800 [==============================] - 262s 328ms/step - loss: 0.2053 - acc: 0.9114 - val_loss: 0.2995 - val_acc: 0.8709\n",
      "Epoch 41/200\n",
      "800/800 [==============================] - 260s 325ms/step - loss: 0.2103 - acc: 0.9092 - val_loss: 0.3013 - val_acc: 0.8628\n",
      "Epoch 42/200\n",
      "800/800 [==============================] - 263s 329ms/step - loss: 0.2027 - acc: 0.9109 - val_loss: 0.2965 - val_acc: 0.8775\n",
      "Epoch 43/200\n",
      "800/800 [==============================] - 262s 328ms/step - loss: 0.2044 - acc: 0.9129 - val_loss: 0.3150 - val_acc: 0.8700\n",
      "Epoch 44/200\n",
      "800/800 [==============================] - 263s 328ms/step - loss: 0.2091 - acc: 0.9080 - val_loss: 0.2885 - val_acc: 0.8744\n",
      "Epoch 45/200\n",
      "800/800 [==============================] - 263s 329ms/step - loss: 0.2085 - acc: 0.9123 - val_loss: 0.3557 - val_acc: 0.8625\n",
      "Epoch 46/200\n",
      "800/800 [==============================] - 256s 320ms/step - loss: 0.2069 - acc: 0.9134 - val_loss: 0.2986 - val_acc: 0.8665\n",
      "Epoch 47/200\n",
      "800/800 [==============================] - 257s 321ms/step - loss: 0.1926 - acc: 0.9183 - val_loss: 0.2888 - val_acc: 0.8722\n",
      "Epoch 48/200\n",
      "800/800 [==============================] - 247s 309ms/step - loss: 0.1926 - acc: 0.9152 - val_loss: 0.3094 - val_acc: 0.8625\n",
      "Epoch 49/200\n",
      "800/800 [==============================] - 247s 308ms/step - loss: 0.1955 - acc: 0.9160 - val_loss: 0.3371 - val_acc: 0.8647\n",
      "Epoch 50/200\n",
      "800/800 [==============================] - 247s 309ms/step - loss: 0.1915 - acc: 0.9195 - val_loss: 0.3357 - val_acc: 0.8700\n",
      "Epoch 51/200\n",
      "800/800 [==============================] - 254s 317ms/step - loss: 0.1853 - acc: 0.9207 - val_loss: 0.3240 - val_acc: 0.8647\n",
      "Epoch 52/200\n",
      "800/800 [==============================] - 281s 351ms/step - loss: 0.1907 - acc: 0.9182 - val_loss: 0.2952 - val_acc: 0.8728\n",
      "Epoch 53/200\n",
      "800/800 [==============================] - 263s 328ms/step - loss: 0.1830 - acc: 0.9237 - val_loss: 0.3041 - val_acc: 0.8690\n",
      "Epoch 54/200\n",
      "800/800 [==============================] - 246s 307ms/step - loss: 0.1829 - acc: 0.9230 - val_loss: 0.3506 - val_acc: 0.8747\n",
      "Epoch 55/200\n",
      "800/800 [==============================] - 248s 311ms/step - loss: 0.1874 - acc: 0.9205 - val_loss: 0.3193 - val_acc: 0.8725\n",
      "Epoch 56/200\n",
      "800/800 [==============================] - 251s 314ms/step - loss: 0.1765 - acc: 0.9248 - val_loss: 0.3182 - val_acc: 0.8709\n",
      "Epoch 57/200\n",
      "800/800 [==============================] - 249s 312ms/step - loss: 0.1758 - acc: 0.9280 - val_loss: 0.3209 - val_acc: 0.8772\n",
      "Epoch 58/200\n",
      "800/800 [==============================] - 249s 312ms/step - loss: 0.1803 - acc: 0.9242 - val_loss: 0.3297 - val_acc: 0.8810\n",
      "Epoch 59/200\n",
      "800/800 [==============================] - 252s 315ms/step - loss: 0.1804 - acc: 0.9255 - val_loss: 0.3486 - val_acc: 0.8684\n",
      "Epoch 60/200\n",
      "800/800 [==============================] - 249s 311ms/step - loss: 0.1768 - acc: 0.9265 - val_loss: 0.3006 - val_acc: 0.8681\n",
      "Epoch 61/200\n",
      "800/800 [==============================] - 246s 308ms/step - loss: 0.1728 - acc: 0.9274 - val_loss: 0.2864 - val_acc: 0.8781\n",
      "Epoch 62/200\n",
      "800/800 [==============================] - 249s 312ms/step - loss: 0.1738 - acc: 0.9276 - val_loss: 0.3358 - val_acc: 0.8653\n",
      "Epoch 63/200\n",
      "800/800 [==============================] - 245s 306ms/step - loss: 0.1704 - acc: 0.9299 - val_loss: 0.3076 - val_acc: 0.8769\n",
      "Epoch 64/200\n",
      "800/800 [==============================] - 245s 306ms/step - loss: 0.1728 - acc: 0.9271 - val_loss: 0.2791 - val_acc: 0.8872\n",
      "Epoch 65/200\n",
      "800/800 [==============================] - 245s 306ms/step - loss: 0.1721 - acc: 0.9305 - val_loss: 0.3140 - val_acc: 0.8810\n",
      "Epoch 66/200\n",
      "800/800 [==============================] - 243s 304ms/step - loss: 0.1662 - acc: 0.9300 - val_loss: 0.2864 - val_acc: 0.8841\n",
      "Epoch 67/200\n",
      "800/800 [==============================] - 250s 313ms/step - loss: 0.1716 - acc: 0.9277 - val_loss: 0.3169 - val_acc: 0.8756\n",
      "Epoch 68/200\n",
      "800/800 [==============================] - 238s 297ms/step - loss: 0.1661 - acc: 0.9316 - val_loss: 0.2942 - val_acc: 0.8875\n",
      "Epoch 69/200\n",
      "800/800 [==============================] - 236s 295ms/step - loss: 0.1582 - acc: 0.9320 - val_loss: 0.3062 - val_acc: 0.8882\n",
      "Epoch 70/200\n",
      "800/800 [==============================] - 238s 298ms/step - loss: 0.1644 - acc: 0.9313 - val_loss: 0.3412 - val_acc: 0.8810\n",
      "Epoch 71/200\n",
      "800/800 [==============================] - 240s 300ms/step - loss: 0.1592 - acc: 0.9324 - val_loss: 0.3141 - val_acc: 0.8775\n",
      "Epoch 72/200\n",
      "800/800 [==============================] - 240s 300ms/step - loss: 0.1650 - acc: 0.9320 - val_loss: 0.2887 - val_acc: 0.8753\n",
      "Epoch 73/200\n",
      "800/800 [==============================] - 236s 295ms/step - loss: 0.1548 - acc: 0.9343 - val_loss: 0.3448 - val_acc: 0.8791\n",
      "Epoch 74/200\n",
      "800/800 [==============================] - 236s 295ms/step - loss: 0.1573 - acc: 0.9347 - val_loss: 0.3210 - val_acc: 0.8781\n",
      "Epoch 75/200\n",
      "800/800 [==============================] - 243s 304ms/step - loss: 0.1584 - acc: 0.9348 - val_loss: 0.3120 - val_acc: 0.8728\n",
      "Epoch 76/200\n",
      "800/800 [==============================] - 237s 296ms/step - loss: 0.1515 - acc: 0.9374 - val_loss: 0.3588 - val_acc: 0.8781\n",
      "Epoch 77/200\n",
      "800/800 [==============================] - 238s 298ms/step - loss: 0.1570 - acc: 0.9348 - val_loss: 0.3030 - val_acc: 0.8791\n",
      "Epoch 78/200\n",
      "800/800 [==============================] - 238s 297ms/step - loss: 0.1501 - acc: 0.9404 - val_loss: 0.2949 - val_acc: 0.8819\n",
      "Epoch 79/200\n",
      "800/800 [==============================] - 238s 297ms/step - loss: 0.1536 - acc: 0.9374 - val_loss: 0.3559 - val_acc: 0.8741\n",
      "Epoch 80/200\n",
      "800/800 [==============================] - 240s 301ms/step - loss: 0.1502 - acc: 0.9385 - val_loss: 0.3175 - val_acc: 0.8784\n",
      "Epoch 81/200\n",
      "800/800 [==============================] - 240s 300ms/step - loss: 0.1465 - acc: 0.9393 - val_loss: 0.3181 - val_acc: 0.8756\n",
      "Epoch 82/200\n",
      "800/800 [==============================] - 249s 311ms/step - loss: 0.1488 - acc: 0.9391 - val_loss: 0.3815 - val_acc: 0.8747\n",
      "Epoch 83/200\n",
      "800/800 [==============================] - 238s 298ms/step - loss: 0.1491 - acc: 0.9396 - val_loss: 0.3122 - val_acc: 0.8813\n",
      "Epoch 84/200\n",
      "800/800 [==============================] - 239s 299ms/step - loss: 0.1450 - acc: 0.9414 - val_loss: 0.3352 - val_acc: 0.8791\n",
      "Epoch 85/200\n",
      "800/800 [==============================] - 236s 295ms/step - loss: 0.1533 - acc: 0.9370 - val_loss: 0.2989 - val_acc: 0.8860\n",
      "Epoch 86/200\n",
      "800/800 [==============================] - 238s 298ms/step - loss: 0.1423 - acc: 0.9411 - val_loss: 0.3820 - val_acc: 0.8810\n",
      "Epoch 87/200\n",
      "800/800 [==============================] - 238s 297ms/step - loss: 0.1424 - acc: 0.9452 - val_loss: 0.4069 - val_acc: 0.8728\n",
      "Epoch 88/200\n",
      "800/800 [==============================] - 234s 292ms/step - loss: 0.1405 - acc: 0.9409 - val_loss: 0.3562 - val_acc: 0.8731\n",
      "Epoch 89/200\n",
      "800/800 [==============================] - 243s 304ms/step - loss: 0.1456 - acc: 0.9414 - val_loss: 0.3454 - val_acc: 0.8778\n",
      "Epoch 90/200\n",
      "800/800 [==============================] - 247s 309ms/step - loss: 0.1394 - acc: 0.9428 - val_loss: 0.3213 - val_acc: 0.8759\n",
      "Epoch 91/200\n",
      "800/800 [==============================] - 240s 300ms/step - loss: 0.1377 - acc: 0.9442 - val_loss: 0.3486 - val_acc: 0.8772\n",
      "Epoch 92/200\n",
      "800/800 [==============================] - 247s 308ms/step - loss: 0.1396 - acc: 0.9447 - val_loss: 0.3098 - val_acc: 0.8741\n",
      "Epoch 93/200\n",
      "800/800 [==============================] - 242s 302ms/step - loss: 0.1331 - acc: 0.9448 - val_loss: 0.3519 - val_acc: 0.8747\n",
      "Epoch 94/200\n",
      "800/800 [==============================] - 238s 298ms/step - loss: 0.1366 - acc: 0.9443 - val_loss: 0.3285 - val_acc: 0.8872\n",
      "Epoch 95/200\n",
      "800/800 [==============================] - 246s 307ms/step - loss: 0.1348 - acc: 0.9449 - val_loss: 0.3326 - val_acc: 0.8857\n",
      "Epoch 96/200\n",
      "800/800 [==============================] - 256s 320ms/step - loss: 0.1264 - acc: 0.9480 - val_loss: 0.3778 - val_acc: 0.8819\n",
      "Epoch 97/200\n",
      "800/800 [==============================] - 264s 331ms/step - loss: 0.1322 - acc: 0.9490 - val_loss: 0.3565 - val_acc: 0.8860\n",
      "Epoch 98/200\n",
      "800/800 [==============================] - 250s 313ms/step - loss: 0.1307 - acc: 0.9459 - val_loss: 0.3277 - val_acc: 0.8734\n",
      "Epoch 99/200\n",
      "800/800 [==============================] - 249s 311ms/step - loss: 0.1349 - acc: 0.9469 - val_loss: 0.3305 - val_acc: 0.8822\n",
      "Epoch 100/200\n",
      "800/800 [==============================] - 249s 312ms/step - loss: 0.1338 - acc: 0.9465 - val_loss: 0.3778 - val_acc: 0.8763\n",
      "Epoch 101/200\n",
      "800/800 [==============================] - 250s 313ms/step - loss: 0.1255 - acc: 0.9496 - val_loss: 0.4299 - val_acc: 0.8584\n",
      "Epoch 102/200\n",
      "800/800 [==============================] - 247s 308ms/step - loss: 0.1342 - acc: 0.9464 - val_loss: 0.3577 - val_acc: 0.8794\n",
      "Epoch 103/200\n",
      "800/800 [==============================] - 253s 316ms/step - loss: 0.1336 - acc: 0.9483 - val_loss: 0.3463 - val_acc: 0.8803\n",
      "Epoch 104/200\n",
      "800/800 [==============================] - 267s 334ms/step - loss: 0.1305 - acc: 0.9477 - val_loss: 0.3479 - val_acc: 0.8844\n",
      "Epoch 105/200\n",
      "800/800 [==============================] - 252s 315ms/step - loss: 0.1291 - acc: 0.9480 - val_loss: 0.3576 - val_acc: 0.8747\n",
      "Epoch 106/200\n",
      "800/800 [==============================] - 252s 316ms/step - loss: 0.1293 - acc: 0.9509 - val_loss: 0.3625 - val_acc: 0.8872\n",
      "Epoch 107/200\n",
      "800/800 [==============================] - 251s 313ms/step - loss: 0.1175 - acc: 0.9545 - val_loss: 0.3621 - val_acc: 0.8794\n",
      "Epoch 108/200\n",
      "800/800 [==============================] - 251s 314ms/step - loss: 0.1225 - acc: 0.9506 - val_loss: 0.3627 - val_acc: 0.8791\n",
      "Epoch 109/200\n",
      "800/800 [==============================] - 251s 314ms/step - loss: 0.1275 - acc: 0.9513 - val_loss: 0.3905 - val_acc: 0.8866\n",
      "Epoch 110/200\n",
      "800/800 [==============================] - 263s 329ms/step - loss: 0.1150 - acc: 0.9531 - val_loss: 0.3720 - val_acc: 0.8863\n",
      "Epoch 111/200\n",
      "800/800 [==============================] - 265s 331ms/step - loss: 0.1284 - acc: 0.9473 - val_loss: 0.3327 - val_acc: 0.8784\n",
      "Epoch 112/200\n",
      "800/800 [==============================] - 249s 311ms/step - loss: 0.1175 - acc: 0.9544 - val_loss: 0.3792 - val_acc: 0.8737\n",
      "Epoch 113/200\n",
      "800/800 [==============================] - 248s 309ms/step - loss: 0.1223 - acc: 0.9511 - val_loss: 0.3773 - val_acc: 0.8791\n",
      "Epoch 114/200\n",
      "800/800 [==============================] - 249s 311ms/step - loss: 0.1261 - acc: 0.9522 - val_loss: 0.3856 - val_acc: 0.8800\n",
      "Epoch 115/200\n",
      "800/800 [==============================] - 255s 318ms/step - loss: 0.1201 - acc: 0.9522 - val_loss: 0.3583 - val_acc: 0.8844\n",
      "Epoch 116/200\n",
      "800/800 [==============================] - 263s 329ms/step - loss: 0.1197 - acc: 0.9543 - val_loss: 0.3363 - val_acc: 0.8878\n",
      "Epoch 117/200\n",
      "800/800 [==============================] - 280s 350ms/step - loss: 0.1215 - acc: 0.9512 - val_loss: 0.3745 - val_acc: 0.8766\n",
      "Epoch 118/200\n",
      "800/800 [==============================] - 263s 328ms/step - loss: 0.1193 - acc: 0.9523 - val_loss: 0.3396 - val_acc: 0.8822\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 246s 308ms/step - loss: 0.1193 - acc: 0.9516 - val_loss: 0.3622 - val_acc: 0.8775\n",
      "Epoch 120/200\n",
      "800/800 [==============================] - 270s 337ms/step - loss: 0.1186 - acc: 0.9543 - val_loss: 0.3721 - val_acc: 0.8897\n",
      "Epoch 121/200\n",
      "800/800 [==============================] - 270s 338ms/step - loss: 0.1169 - acc: 0.9551 - val_loss: 0.3648 - val_acc: 0.8797\n",
      "Epoch 122/200\n",
      "800/800 [==============================] - 244s 305ms/step - loss: 0.1121 - acc: 0.9560 - val_loss: 0.3512 - val_acc: 0.8819\n",
      "Epoch 123/200\n",
      "800/800 [==============================] - 257s 322ms/step - loss: 0.1192 - acc: 0.9545 - val_loss: 0.4194 - val_acc: 0.8725\n",
      "Epoch 124/200\n",
      "800/800 [==============================] - 266s 333ms/step - loss: 0.1201 - acc: 0.9538 - val_loss: 0.3759 - val_acc: 0.8794\n",
      "Epoch 125/200\n",
      "800/800 [==============================] - 289s 361ms/step - loss: 0.1174 - acc: 0.9537 - val_loss: 0.3492 - val_acc: 0.8759\n",
      "Epoch 126/200\n",
      "800/800 [==============================] - 272s 340ms/step - loss: 0.1110 - acc: 0.9588 - val_loss: 0.4193 - val_acc: 0.8825\n",
      "Epoch 127/200\n",
      "800/800 [==============================] - 277s 347ms/step - loss: 0.1103 - acc: 0.9570 - val_loss: 0.4212 - val_acc: 0.8766\n",
      "Epoch 128/200\n",
      "800/800 [==============================] - 267s 334ms/step - loss: 0.1185 - acc: 0.9528 - val_loss: 0.3756 - val_acc: 0.8813\n",
      "Epoch 129/200\n",
      "438/800 [===============>..............] - ETA: 1:53 - loss: 0.1143 - acc: 0.9545"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-40cff5396449>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                     \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                     validation_steps = 200)\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1154\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1156\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2075\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2076\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2077\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2078\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2079\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1795\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1796\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1797\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1798\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1799\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2332\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2333\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(train,\n",
    "                    steps_per_epoch = 800,\n",
    "                    epochs = 200,\n",
    "                    validation_data = test,\n",
    "                    validation_steps = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_test = []\n",
    "for ts in islice(test, 50):\n",
    "    X_test.append(ts[0])\n",
    "    y_test.append(ts[1])\n",
    "\n",
    "X_test = np.concatenate(X_test)\n",
    "y_test = np.concatenate(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.argwhere(y_test != y_pred).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(X_test[14])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
